{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('C:\\wzl\\实习\\data')\n",
    "\n",
    "feature_list = ['lat', 'lon',\n",
    "                        'TMP_P0_L1_GLL0', 'SPFH_P0_2L108_GLL0', 'RH_P0_L4_GLL0',\n",
    "                        'PWAT_P0_L200_GLL0', 'UGRD_P0_L6_GLL0', 'GUST_P0_L1_GLL0',\n",
    "                        'PRES_P0_L7_GLL0', 'CultivatedLand', 'WoodLand', 'GrassLand', 'Waters',\n",
    "                        'UrbanRural', 'UnusedLand', 'Ocean', 'ELEVATION', 'AOD', 'month',\n",
    "                        'year', 'weekday']\n",
    "label_name = 'o3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "train = pd.read_csv('day\\\\train_set_daily_new.csv')\n",
    "test = pd.read_csv('day\\\\test_set_daily_new.csv')\n",
    "\n",
    "X_train, y_train, X_test, y_test = train[feature_list], train[label_name],test[feature_list],test[label_name]\n",
    "X_train = pd.get_dummies(X_train,columns = ['month','year','weekday'],drop_first=True)\n",
    "X_test = pd.get_dummies(X_test,columns = ['month','year','weekday'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mynet(nn.Module):\n",
    "    def __init__(self,input_size,hidden1,hidden2,hidden3,hidden4,output_size,dropout_prob):\n",
    "        super(Mynet,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1,hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden2,hidden3)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden3)\n",
    "        self.fc4 = nn.Linear(hidden3,hidden4)\n",
    "        self.fc5 = nn.Linear(hidden4,output_size)\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        feature1 = x\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "    \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        feature2 = x\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        feature3 = x\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        feature4 = x\n",
    "        x = F.relu(x)\n",
    "        out = self.fc5(x)\n",
    "        return out,feature1,feature2,feature3,feature4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "device = torch.device('cuda')\n",
    "input = X_train.shape[1]\n",
    "Mymodel = Mynet(input_size=input,hidden1 = 1680, hidden2 = 2560, hidden3 = 128, hidden4 = 64,output_size=1,\n",
    "                dropout_prob=0.2).to(device)\n",
    "X_train = torch.tensor(X_train, dtype = torch.float32, device = device)\n",
    "X_test = torch.tensor(X_test, dtype = torch.float32, device = device)\n",
    "y_train = torch.tensor(y_train, dtype = torch.float32, device = device)\n",
    "y_test = torch.tensor(y_test, dtype = torch.float32, device = device)\n",
    "\n",
    "train = TensorDataset(X_train, y_train)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_data = DataLoader(train, batch_size = 4096, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 loss tensor(648.7390, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m300\u001b[39m):\n\u001b[0;32m      5\u001b[0m     loss_list\u001b[39m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m X,y \u001b[39min\u001b[39;00m train_data:\n\u001b[0;32m      7\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m         out \u001b[39m=\u001b[39m Mymodel(X)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\miniconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\miniconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\miniconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\miniconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\miniconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\dataset.py:196\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(tensor[index] \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors)\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\miniconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\dataset.py:196\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Mymodel.parameters(),lr = 0.01)\n",
    "Loss = nn.MSELoss()\n",
    "Mymodel.train()\n",
    "for i in range(300):\n",
    "    loss_list= []\n",
    "    for X,y in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        out = Mymodel(X)[0]\n",
    "        y = y.view(-1,1)\n",
    "        loss = Loss(out,y)\n",
    "        loss_list.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('episode',i,'loss',sum(loss_list)/len(loss_list))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mynet(\n",
       "  (fc1): Linear(in_features=37, out_features=1680, bias=True)\n",
       "  (bn1): BatchNorm1d(1680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=1680, out_features=2560, bias=True)\n",
       "  (bn2): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=2560, out_features=128, bias=True)\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc5): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.save(Mymodel,'day\\\\NN5.pth')\n",
    "Mymodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.844414\n",
      "SMAPE: 18.140986561775208\n",
      "MAE: 9.57674\n",
      "R2: 0.7947686167271085\n",
      "confidence 0.6307052691832615\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "Mymodel = torch.load('day\\\\NN5.pth')\n",
    "\n",
    "Mymodel.eval()\n",
    "y_pred = Mymodel(X_test)[0].cpu().detach().numpy().flatten()\n",
    "#y_test = y_test.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "smape = np.mean(2 * np.abs(y_pred - y_test) / (np.abs(y_pred) + np.abs(y_test))) * 100\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "from confidence import confidence\n",
    "conf = confidence(y_test,y_pred,10)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"SMAPE:\", smape)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)\n",
    "print('confidence',conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
